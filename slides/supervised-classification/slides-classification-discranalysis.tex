\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Discriminant Analysis
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/disc_analysis-qda_1
}{% Learning goals, wrapped inside itemize environment
  \item Understand the ideas of linear and quadratic discriminant analysis
  \item Understand how parameteres are estimated for LDA and QDA
  \item Understand how decision boundaries are computed for LDA and QDA
}

\begin{vbframe}{Linear discriminant analysis (LDA)}

\begin{small}
LDA follows a generative approach, employing Bayes' theorem:

$$\pikx = \postk = \frac{\P(\xv | y = k) \P(y = k)}{\P(\xv)} = \frac{\pdfxyk \pik}{\sumjg \pdfxyk[j] \pi_j},$$

where we now have to pick a distributional form for $\pdfxyk$. LDA assumes that each class density is modeled as a \emph{multivariate Gaussian}:

$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\xv - \muk)^T \Sigma^{-1} (\xv - \muk)\right)
$$

with equal covariance, i. e. $\Sigma_k = \Sigma \quad \forall k$.
\end{small}

\vspace{-0.9em}
\begin{center}
\includegraphics[width=0.60\textwidth, clip=true, trim={0 75 0 45}]{figure/disc_analysis-lda_1.png}
\end{center}
\end{vbframe}

\begin{vbframe}{LDA parameter estimation}
  
Parameters $\thetav$ are estimated in a straightforward manner by
\begin{eqnarray*}
\hat{\pik} &=& \frac{n_k}{n},\text{ where $n_k$ is the number of class-$k$ observations} \\
\hat{\muk} &=& \frac{1}{n_k}\sum_{i: \yi = k} \xi \\
\hat{\Sigma} &=& \frac{1}{n - g} \sumkg \sum_{i: \yi = k} (\xi - \hat{\muk}) (\xi - \hat{\muk}) ^T
\end{eqnarray*}

\vspace{-0.9em}
\begin{center}
\includegraphics[width=0.70\textwidth, clip=true, trim={0 75 0 45}]{figure/disc_analysis-lda_2.png}
\end{center}

\end{vbframe}

\begin{vbframe}{LDA decision boundaries}

Because of the equal covariance structure of all class-specific Gaussian, the decision boundaries of LDA are linear.

\begin{center}
\includegraphics[width=\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_db-lda.png}
\end{center}

\end{vbframe}

\begin{vbframe}{LDA as linear classifier}

We can formally show that LDA is a linear classifier, by showing that the posterior probabilities
can be written as linear scoring functions - up to any isotonic / rank-preserving transformation.

$$
  \pikx = \frac{\pik \cdot \pdfxyk }{\pdfx} = \frac{\pik \cdot \pdfxyk}{\sumjg \pi_j \cdot \pdfxyk[j]}
$$

As the denominator is the same for all classes we only need to consider 
$$\pik \cdot \pdfxyk$$ 
and show that this can be written as a linear function of $\xv$.
  
\end{vbframe}

\begin{vbframe}{Proofing linearity of LDA}
\begin{eqnarray*}
& \pik \cdot \pdfxyk \\
  \propto& \pik \exp\left(- \frac{1}{2} \xv^T\Sigma^{-1}\xv - \frac{1}{2} \muk^T \Sigma^{-1} \muk + \xv^T \Sigma^{-1} \muk \right) \\
=& \exp\left(\log \pik  - \frac{1}{2} \muk^T \Sigma^{-1} \muk + \xv^T \Sigma^{-1} \muk \right) \exp\left(- \frac{1}{2} \xv^T\Sigma^{-1}\xv\right) \\
=& \exp\left(\thetav_{0k} + \xv^T \thetav_k\right) \exp\left(- \frac{1}{2} \xv^T\Sigma^{-1}\xv\right)\\
\propto& \exp\left(\thetav_{0k} + \xv^T \thetav_k\right) 
\end{eqnarray*}

by defining
$\thetav_{0k} := \log \pik  - \frac{1}{2} \muk^T \Sigma^{-1} \muk$ $\quad$ and $\thetav_k := \Sigma^{-1} \muk$.

\lz

We have again left out all constants which are the same for all classes $k$, 
  so the normalizing constant of our Gaussians and 
  $\exp\left(- \frac{1}{2} \xv^T\Sigma^{-1}\xv\right)$.

\lz

By finally taking the log, we can write our transformed scores as linear:  

$$ \fkx =  \thetav_{0k} + \xv^T \thetav_k $$

\end{vbframe}


\begin{vbframe}{Quadratic discriminant analysis (QDA)}

While LDA assumes equal covariance $\Sigma$, QDA lifts this restriction. It is therefore a direct \textbf{generalization of LDA}, only assuming class densities are Gaussians with \textit{unequal} covariances $\Sigma_k$.
$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma_k|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\xv - \muk)^T \Sigma_k^{-1} (\xv - \muk)\right)
$$

$\Rightarrow$ Better data fit but \textbf{requires estimation of more parameters} ($\Sigma_k$)!

\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.9\textwidth, clip=true, trim={0 75 0 45}]{figure/disc_analysis-qda_1.png}
\end{center}

\end{vbframe}

\begin{vbframe}{QDA parameter estimation}

Parameters are estimated in a straightforward manner by:\\
\vspace{-0.9em}
\begin{eqnarray*}
\hat{\pik} &=& \frac{n_k}{n},\text{ where $n_k$ is the number of class-$k$ observations} \\
\hat{\muk} &=& \frac{1}{n_k}\sum_{i: \yi = k} \xi \\
\hat{\Sigma_k} &=& \frac{1}{n_k - 1} \sum_{i: \yi = k} (\xi - \hat{\muk}) (\xi - \hat{\muk})^T \\
\end{eqnarray*}


\vspace{-0.9em}
\begin{center}
\includegraphics[width=0.7\textwidth, clip=true, trim={0 75 0 45}]{figure/disc_analysis-qda_2.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Parameter estimation example}
\begin{small}
E.g., for a simple two-class, 2-dimensional dataset: \\
Class 1: $\mathbf{x}_1 = \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \mathbf{x}_2 = \begin{pmatrix} 2 \\ 3 \end{pmatrix}, \mathbf{x}_3 = \begin{pmatrix} 3 \\ 4 \end{pmatrix}$\\
Class 2: $\mathbf{x}_4 = \begin{pmatrix} 6 \\ 8 \end{pmatrix}, \mathbf{x}_5 = \begin{pmatrix} 7 \\ 9 \end{pmatrix}, \mathbf{x}_6 = \begin{pmatrix} 8 \\ 10 \end{pmatrix}$

\[
\hat{\pi}_1 = \frac{n_1}{n} = \frac{3}{6} = 0.5, \quad \hat{\pi}_2 = \frac{n_2}{n} = \frac{3}{6} = 0.5
\]

\[
\hat{\mu}_1 = \frac{1}{3} \left( \begin{pmatrix} 1 \\ 2 \end{pmatrix} + \begin{pmatrix} 2 \\ 3 \end{pmatrix} + \begin{pmatrix} 3 \\ 4 \end{pmatrix} \right) = \begin{pmatrix} 2 \\ 3 \end{pmatrix}, \hat{\mu}_2 = \frac{1}{3} \left( \begin{pmatrix} 6 \\ 8 \end{pmatrix} + \begin{pmatrix} 7 \\ 9 \end{pmatrix} + \begin{pmatrix} 8 \\ 10 \end{pmatrix} \right) = \begin{pmatrix} 7 \\ 9 \end{pmatrix}
\]

\[
\hat{\Sigma}_1 = \frac{1}{2} \left( (\mathbf{x}_1 - \hat{\mu}_1)(\mathbf{x}_1 - \hat{\mu}_1)^T + (\mathbf{x}_2 - \hat{\mu}_1)(\mathbf{x}_2 - \hat{\mu}_1)^T + (\mathbf{x}_3 - \hat{\mu}_1)(\mathbf{x}_3 - \hat{\mu}_1)^T \right)
\]
\[
= \frac{1}{2} \begin{pmatrix} 2 & 2 \\ 2 & 2 \end{pmatrix} = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}, \hat{\Sigma}_2 = \frac{1}{2} \begin{pmatrix} 2 & 2 \\ 2 & 2 \end{pmatrix} = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}
\]
\end{small}
\end{vbframe}

\begin{vbframe}{QDA decision boundaries}

\vspace{-2em}
\begin{small}
\begin{eqnarray*}
\pikx &\propto& \pik \cdot \pdfxyk \\
&\propto& \pik |\Sigma_k|^{-\frac{1}{2}}\exp(- \frac{1}{2} \xv^T\Sigma_k^{-1}\xv - \frac{1}{2} \muk^T \Sigma_k^{-1} \muk + \xv^T \Sigma_k^{-1} \muk )
\end{eqnarray*}

Taking the log of the above, we can define a discriminant function that is quadratic in $x$.

$$ \log \pik - \frac{1}{2} \log |\Sigma_k| - \frac{1}{2} \muk^T \Sigma_k^{-1} \muk + \xv^T \Sigma_k^{-1} \muk - \frac{1}{2} \xv^T\Sigma_k^{-1}\xv $$


Allowing for quadratic decision boundaries:

\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.68\textwidth, clip=true, trim={0 0 0 20}]{figure/disc_db-qda.png}
\end{center}
\end{small}

\end{vbframe}

\begin{vbframe}{Discriminant analysis comparison}
Due to requiring less parameter estimates, LDA might be preferable over QDA in higher dimensions. Measuring the classification error on some toy data:

\begin{center}
\includegraphics[width=\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_cod.png}
\end{center}

\end{vbframe}
\endlecture

\end{document}

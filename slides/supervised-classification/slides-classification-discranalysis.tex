\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Discriminant Analysis
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/disc_analysis-qda_1
}{% Learning goals, wrapped inside itemize environment
  \item Understand the ideas of linear and quadratic discriminant analysis
  \item Understand how parameteres are estimated for LDA and QDA
  \item Understand how decision boundaries are computed for LDA and QDA
}

\begin{vbframe}{Linear discriminant analysis (LDA)}

\begin{small}
LDA follows a generative approach, employing Bayes' theorem:

$$\pikx = \postk = \frac{\P(\xv | y = k) \P(y = k)}{\P(\xv)} = \frac{\pdfxyk \pik}{\sumjg \pdfxyk[j] \pi_j},$$

where we now have to pick a distributional form for $\pdfxyk$. LDA assumes that each class density is modeled as a \emph{multivariate Gaussian}:

$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\xv - \muk)^T \Sigma^{-1} (\xv - \muk)\right)
$$

with equal covariance, i. e. $\Sigma_k = \Sigma \quad \forall k$.
\end{small}

\vspace{-0.9em}
\begin{center}
\includegraphics[width=0.60\textwidth, clip=true, trim={0 75 0 45}]{figure/disc_analysis-lda_2.png}
\end{center}
\end{vbframe}

\begin{vbframe}{Univariate example}
\begin{small}
\begin{itemize}
\item Given the heights (in cm) of 10 men and 10 women, we aim to classify a new person as male or female based on their height.
\item LDA models both classes using normal distributions with equal standard deviations (identical shapes).
\end{itemize}
\begin{center}
\includegraphics[width=0.95\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_univariate-1.png}
\end{center}
\centerline{The optimal separation is located at the intersection (= decision boundary)!}
\end{small}
\end{vbframe}

\begin{vbframe}{Univariate example: equal class sizes}
\begin{small}
Using our learned distributions, we can compute the posterior probability that a 172 cm tall person is male.
\begin{center}
\includegraphics[width=0.85\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_univariate-2.png}
\end{center}
For equal class sizes, the prior probs $\pik$ cancel out (since $\pi_{man} = \pi_{woman}$):
$$
\P(y = \text{man} \mid \xv) = \frac{p(\xv \mid y = \text{man})}{p(\xv \mid y = \text{man}) + p(\xv \mid y = \text{woman})} = \frac{0.0135}{0.0135 + 0.088} = 0.133
$$
\end{small}
\end{vbframe}

\begin{vbframe}{Univariate example: unequal class sizes}
\begin{small}
For unequal class sizes (e.g., $\pi_{woman} = 2\pi_{man}$), the prior probs matter and cause a shift of the decision boundary towards the smaller class.
\begin{center}
\includegraphics[width=0.86\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_univariate-3.png}
\end{center}
\begin{align*}
\P(y = \text{man} \mid \xv) &= \frac{p(\xv \mid y = \text{man}) \pi_{man}}{p(\xv \mid y = \text{man}) \pi_{man} + p(\xv \mid y = \text{woman}) \pi_{woman}}\\
&=\frac{0.0135 \tfrac{1}{3}}{0.0135 \tfrac{1}{3} + 0.088 \tfrac{2}{3}} = 0.0712
\end{align*}

\end{small}

\end{vbframe}
\begin{vbframe}{LDA decision boundaries}

Because of the equal covariance structure of all class-specific Gaussians, the decision boundaries of LDA are always linear.

\begin{center}
\includegraphics[width=\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_db-lda.png}
\end{center}

\end{vbframe}

\begin{vbframe}{LDA as linear classifier}

We can formally show that LDA is a linear classifier, by showing that the posterior probabilities
can be written as linear scoring functions - up to any isotonic / rank-preserving transformation.

$$
  \pikx = \frac{\pik \cdot \pdfxyk }{\pdfx} = \frac{\pik \cdot \pdfxyk}{\sumjg \pi_j \cdot \pdfxyk[j]}
$$

As the denominator is the same for all classes we only need to consider 
$$\pik \cdot \pdfxyk$$ 
and show that this can be written as a linear function of $\xv$.

\end{vbframe}

\begin{vbframe}{Proving linearity of LDA}
\begin{eqnarray*}
& \pik \cdot \pdfxyk \\
\propto& \textcolor{orange}{\pik} \exp\left(\textcolor{blue}{- \frac{1}{2} \xv^T\Sigma^{-1}\xv} \textcolor{orange}{- \frac{1}{2} \muk^T \Sigma^{-1} \muk} + \xv^T \textcolor{purple}{\Sigma^{-1} \muk} \right) \\
=& \exp\left(\textcolor{orange}{\log \pik - \frac{1}{2} \muk^T \Sigma^{-1} \muk} + \xv^T \textcolor{purple}{\Sigma^{-1} \muk} \right) \exp\left(\textcolor{blue}{- \frac{1}{2} \xv^T\Sigma^{-1}\xv}\right) \\
=& \exp\left(\textcolor{orange}{\thetav_{0k}} + \xv^T \textcolor{purple}{\thetav_k}\right) \exp\left(\textcolor{blue}{- \frac{1}{2} \xv^T\Sigma^{-1}\xv}\right)\\
\propto& \exp\left(\thetav_{0k} + \xv^T \thetav_k\right) 
\end{eqnarray*}

by defining
$\thetav_{0k} := \log \pik  - \frac{1}{2} \muk^T \Sigma^{-1} \muk$ $\quad$ and $\thetav_k := \Sigma^{-1} \muk$.

\lz

We have again left out all constants which are the same for all classes $k$, so the normalizing constant of our Gaussians and $\exp\left(- \frac{1}{2} \xv^T\Sigma^{-1}\xv\right)$.

\lz

By finally taking the log, we can write our transformed scores as linear:  

$$ \fkx =  \thetav_{0k} + \xv^T \thetav_k $$

\end{vbframe}


\begin{vbframe}{Quadratic discriminant analysis (QDA)}

While LDA assumes equal covariance $\Sigma$, QDA lifts this restriction. It is therefore a direct \textbf{generalization of LDA}, only assuming class densities are Gaussians with \textit{unequal} covariances $\Sigma_k$.
$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma_k|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\xv - \muk)^T \Sigma_k^{-1} (\xv - \muk)\right)
$$

$\Rightarrow$ Better data fit but \textbf{requires estimation of more parameters} ($\Sigma_k$)!

\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.9\textwidth, clip=true, trim={0 75 0 45}]{figure/disc_analysis-qda_2.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Univariate Example with QDA}
\begin{small}
Diï¬€erent covariance matrices lead to multiple classification rules:
\begin{itemize}
  \item $x < 159.6$ is being assigned to class \textit{man}.
  \item $159.6 < x < 175.5$ is being assigned to class \textit{woman}.
  \item $x > 175.5$ is being assigned to class \textit{man}.
\end{itemize}
\begin{center}
\includegraphics[width=0.8\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_univariate-4.png}
\end{center}
$\Rightarrow$ The separation function is quadratic, we learn a curved decision boundary.
\end{small}
\end{vbframe}

\begin{vbframe}{QDA decision boundaries}

\vspace{-2em}
\begin{small}
\begin{eqnarray*}
\pikx &\propto& \pik \cdot \pdfxyk \\
&\propto& \pik |\Sigma_k|^{-\frac{1}{2}}\exp(- \frac{1}{2} \xv^T\Sigma_k^{-1}\xv - \frac{1}{2} \muk^T \Sigma_k^{-1} \muk + \xv^T \Sigma_k^{-1} \muk )
\end{eqnarray*}

Taking the log of the above, we can define a discriminant function that is quadratic in $x$:

$$ \log \pik - \frac{1}{2} \log |\Sigma_k| - \frac{1}{2} \muk^T \Sigma_k^{-1} \muk + \xv^T \Sigma_k^{-1} \muk - \frac{1}{2} \xv^T\Sigma_k^{-1}\xv $$


Allowing for curved decision boundaries:

\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.68\textwidth, clip=true, trim={0 0 0 20}]{figure/disc_db-qda.png}
\end{center}
\end{small}

\end{vbframe}

\begin{vbframe}{Parameter estimation}

Parameters $\thetav$ are estimated in a straightforward manner by:\\
\begin{equation*}
\begin{aligned}
\pikh &= \frac{n_k}{n},\text{ where $n_k$ is the number of class-$k$ observations} \\
\mukh &= \frac{1}{n_k}\sum_{i:\yi = k} \xi \\
\Sigmah_k &= \frac{1}{n_k - 1} \sum_{i: \yi = k} (\xi - \mukh) (\xi - \mukh)^T \quad \quad \text{   (QDA)} \\
\Sigmah &= \frac{1}{n - g} \sumkg \sum_{i: \yi = k} (\xi - \mukh) (\xi - \mukh)^T \quad \text{(LDA)} \\
\end{aligned}
\end{equation*}

\lz

As $\Sigmah_k, \Sigmah$ are $p \times p$ matrices (for $p$ features), estimating all $\Sigmah_k$ involves $\frac{p(p+1)}{2} \cdot g$ parameters across $g$ classes (vs. just $\frac{p(p+1)}{2}$ for LDA's $\Sigmah$).
\end{vbframe}

\begin{vbframe}{QDA parameter estimation example}
\begin{small}
E.g., for a simple two-class, 2-dimensional dataset:\\

Class 1: $\xv_1 = \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \xv_2 = \begin{pmatrix} 2 \\ 3 \end{pmatrix} $, 
Class 2: $\xv_3 = \begin{pmatrix} 6 \\ 8 \end{pmatrix}, \xv_4 = \begin{pmatrix} 7 \\ 9 \end{pmatrix}, \xv_5 = \begin{pmatrix} 8 \\ 10 \end{pmatrix}$

\lz

Class priors: $\pikh[1] = \frac{n_1}{n} = \frac{2}{5} = 0.4, \quad \pikh[2] = \frac{n_2}{n} = \frac{3}{5} = 0.6$

Class means: $\mukh[1] = \frac{1}{2} \left( \xv_1 + \xv_2 \right) = \begin{pmatrix} 1.5 \\ 2.5 \end{pmatrix}, \quad \mukh[2] = \frac{1}{3} \left( \xv_3 + \xv_4 + \xv_5 \right) = \begin{pmatrix} 7 \\ 9 \end{pmatrix}$

Class covariances: $(\xv_1 - \mukh[1])(\xv_1 - \mukh[1])^\top = \begin{pmatrix} 0.25 & 0.25 \\ 0.25 & 0.25 \end{pmatrix} = (\xv_2 - \mukh[1])(\xv_2 - \mukh[1])^\top$\\ 
$\Rightarrow \Sigmah_1 = \frac{1}{1} \left(\begin{pmatrix} 0.25 & 0.25 \\ 0.25 & 0.25 \end{pmatrix} + \begin{pmatrix} 0.25 & 0.25 \\ 0.25 & 0.25 \end{pmatrix} \right) = \begin{pmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \end{pmatrix}$

$(\xv_3 - \mukh[2])(\xv_3 - \mukh[2])^\top = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} = (\xv_5 - \mukh[2])(\xv_5 - \mukh[2])^\top$, $(\xv_4 - \mukh[2])(\xv_4 - \mukh[2])^\top = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix}$\\
$\Rightarrow \Sigmah_2 = \frac{1}{2} \left( \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} + \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} + \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} \right) = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}$
\end{small}
\end{vbframe}


\begin{vbframe}{Discriminant analysis comparison}
\begin{small}
Measuring the classification error on a toy binary classification task of increasing dimension, where data for each class are drawn from a multivariate normal distribution with the same mean and slight variations in covariance, followed by a small shift in up to 5 features for one class to create structure:
\end{small}

\begin{center}
\includegraphics[width=0.8\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_cod.png}
\end{center}

$\Rightarrow$ LDA might be preferable over QDA in higher dimensions!

\end{vbframe}
\endlecture

\end{document}

\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Logistic Regression
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/reg_class_log_7
}{% Learning goals, wrapped inside itemize environment
  \item Understand the definition of the logit model
  \item Understand how a reasonable loss function for binary classification can be derived
  \item Know the hypothesis space that belongs to the logit model
}

\framebreak


\begin{vbframe}{Motivation}

A \textbf{discriminant} approach for directly modeling the posterior probabilities $\pixt$ of the labels is \textbf{logistic regression}. 

For now, let's focus on the binary case $y \in \setzo$ and use empirical risk minimization.
  
$$ \argmin_{\thetav \in \Theta} \risket = \argmin_{\thetav \in \Theta} \sumin \Lpixyit.$$

\lz
A naive approach would be to model
\[
\pixt = \thx .
\]

NB: We will often suppress the intercept in notation.

\lz

Obviously this could result in predicted probabilities $\pixt \not\in [0,1]$.

\end{vbframe}

\begin{vbframe}{Restricting the hypothesis space}

To avoid this, logistic regression \enquote{squashes} the estimated linear scores $\thx$ to $[0,1]$ through the \textbf{logistic function} $s$:
\[
\pixt = \frac{\exp\left( \thx \right)}{1+\exp\left(\thx \right)} = \frac{1}{1+\exp\left( -\thx \right)} = s\left( \thx \right)
\]

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.70\textwidth]{figure/reg_class_log_1} 

}

\end{knitrout}

Thus, the \textbf{Hypothesis Space} of logistic regression is defined as:
\begin{eqnarray*}
  \Hspace = \left\{\pi: \Xspace \to [0,1] ~|~\pix = s(\thx) \right\}
\end{eqnarray*}

\end{vbframe}

\begin{vbframe}{Logistic Function}
  
The intercept shifts $s(f)$ horizontally $s(\theta_0 + f) = \frac{\exp(\theta_0 + f)}{1+\exp(\theta_0 + f)}$
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_log_2}  

}



\end{knitrout}

Scaling $f$ like $s(\alpha f) = \frac{\exp(\alpha f)}{1+\exp(\alpha f)}$ controls the slope and direction.
\lz
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_log_3} 

}



\end{knitrout}

\end{vbframe}

\begin{vbframe}{Deriving a loss function}

We need to find a suitable loss function to use \textbf{ERM}. Starting from the likelihood function $\LL$ for the binary case:
$$
\LLt = \prod_{i \text{ with } \yi = 1} \pixit * \prod_{i \text{ with } \yi = 0} (1-\pixit)
$$
Taking the log to convert products into sums:
$$
\loglt = \log \LLt = \sum_{i \text{ with } \yi = 1} \log(\pixit) + \sum_{i \text{ with } \yi = 0} \log(1-\pixit)
$$
Since we want to minimize our risk, we work with the negative $\loglt$:
$$
- \loglt = - \sum_{i \text{ with } \yi = 1} \log(\pixit) - \sum_{i \text{ with } \yi = 0} \log(1-\pixit)
$$
The resulting loss $\Lpixy = -y\log(\pix)-(1-y)\log(1-\pix)$ is called the \textbf{Bernoulli, log} or \textbf{cross-entropy} loss.
\end{vbframe}

\begin{vbframe}{Bernoulli / Log Loss}

Graphically:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_log_4}  

}

\end{knitrout}
The \textbf{log loss}
\begin{itemize}
  \item penalizes confidently wrong predictions heavily
  \item is used for many other classifiers, e.g., in NNs or boosting 
  \item has no analytical solution for \textbf{optimization}! Thus, we use \textbf{numerical optimization}, typically gradient-based methods to fit a logistic regression model.
\end{itemize}


\end{vbframe}

\begin{vbframe}{Logistic Regression in 1D}
With one feature $\xv \in \R$. The figure shows data and $\xv \mapsto \pix$.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_log_5} 

}

\end{knitrout}
\end{vbframe}
\begin{vbframe}{Logistic Regression in 2D}

Obviously, logistic regression is a linear classifier, as $\pixt = s\left( \thx \right)$ 
and $s$ is isotonic.

\lz
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_log_6}  

}

\end{knitrout}

\framebreak

\begin{columns}[T]
\begin{column}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure_man/logreg-2vars-surface.png}
\end{column}
\begin{column}{0.5\textwidth}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_log_7} 

}

\end{knitrout}
\end{column}
\end{columns}

\end{vbframe}

\endlecture

\end{document}

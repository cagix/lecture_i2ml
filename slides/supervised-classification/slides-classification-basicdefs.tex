\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Basic Definitions
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/reg_class_bdefs
}{% Learning goals, wrapped inside itemize environment
  \item Understand why classification models have a score / probability as output and not a class
  \item Understand the difference between scoring and probabilistic classifiers
  \item Know the concept of decision regions and boundaries
  \item Know the difference between generative and discriminant approach
}


\begin{vbframe}{Classification Tasks}

\begin{itemize}
\item In classification, we aim at predicting a discrete output 

$$
y \in \Yspace = \{C_1, ..., C_g\}
$$

with $2 \le g < \infty$, given data $\D$ and assume the classes to be encoded as $\Yspace = \setzo$ or $\Yspace = \setmp$ (in the \textbf{binary case} $g = 2$) and $\Yspace = \gset$  (in the \textbf{multiclass case} $g \ge 3$)

\item We use \textbf{one-hot encoding} $o(y)$ for the g-length encoding vector with $o_k(y) = \I(y = k) \in \{0,1\}$ to represent classes numerically:

\end{itemize}

\begin{center}
  \includegraphics[width=0.65\linewidth]{figure_man/one-hot_encoding.png} 
\end{center}

\end{vbframe}


\begin{vbframe}{Classification Models} 
We defined models $f: \Xspace \to \R^g$ as functions that output (continuous) \textbf{scores} / \textbf{probabilities} and \textbf{not} (discrete) classes. Why? 

\begin{itemize}
  \item From an optimization perspective, it is \textbf{much} (!) easier to optimize costs for continuous-valued functions 
  \item Scores / probabilities (for classes) contain more information than the class labels alone
  \item As we will see later, scores can easily be transformed into class labels; but class labels cannot be transformed into scores
\end{itemize}

We distinguish \textbf{scoring} and \textbf{probabilistic} classifiers.
\end{vbframe}


\begin{vbframe}{Scoring Classifiers}
\begin{itemize}
% \item Scoring classifiers assume the output variable to be -1/+1-encoded, i. e. $\Yspace = \{-1, 1\}$
\item Construct $g$ \textbf{discriminant} / \textbf{scoring functions} $f_1, ..., f_g: \Xspace \to \R$
\item Scores $f_1(\xv), \ldots, \fkx[g]$ are transformed into classes by choosing the class with the maximum score 
$$
h(\xv) = \argmax_{k \in \gset} \fkx[k]. 
$$ 

\item For $g = 2$, a single discriminant function $\fx = f_{1}(\xv) - f_{-1}(\xv)$ is sufficient (note that it would be natural here to label the classes with $\setmp$), class labels are constructed by $\hx = \text{sgn}(\fx)$
\item $|\fx|$ is called \enquote{confidence}
\end{itemize}

\vspace{-0.3cm}

\begin{center}
  \includegraphics[clip=true, trim={0 60 0 30}]{figure_man/scores.png} 
\end{center}
 
\end{vbframe}

\begin{vbframe}{Probabilistic Classifiers}
\begin{itemize}
% \item Probabilistic classifiers assume the output variable to be 0/1-encoded, i. e. $\Yspace = \{0, 1\}$
\item Construct $g$ \textbf{probability functions} $\pi_1, ..., \pi_g: \Xspace \to [0, 1],~\sum_i \pi_i = 1$ 
\item Probabilities $\pi_1(\xv), \ldots, \pikx[g]$ are transformed into labels by predicting the class with the maximum probability
$$
\hx = \argmax_{k \in \gset} \pikx
$$ 
\item For $g = 2$ one $\pix$ is constructed (note that it would be natural here to label the classes with $\setzo$)

\item Probabilistic classifiers can also be seen as scoring classifiers

% \item If we want to emphasize that our model outputs probabilities, we denote the model as $\pix: \Xspace \to [0, 1]^g$; if we are talking about models in a general sense, we write $f$, comprising both probabilistic and scoring classifiers (context will make this clear!) 
\end{itemize}


\begin{center}
  \includegraphics[clip=true, trim={0 60 0 30}]{figure_man/probabilities.png} 
\end{center}
\end{vbframe}


\begin{frame}{Thresholding}
  
\begin{itemize}
\item Both scoring and probabilistic classifiers can also output classes by thresholding
\item Thresholding: $\hx:= [\pix \ge c]$ or $\hx = [\fx \ge c]$ for some threshold $c$.
\item Usually $c = 0.5$ for probabilistic, $c = 0$ for scoring classifiers.
\item There are also versions of thresholding for the multiclass case

\end{itemize}

\only<1>{
  \begin{center}
  \includegraphics[clip=true, trim={0 390 310 390}, width=0.9\linewidth]{figure_man/threshold_1.png}
\end{center}
}

\only<2>{
  \begin{center}
  \includegraphics[clip=true, trim={0 390 310 390}, width=0.9\linewidth]{figure_man/threshold_2.png}
  \end{center}
  }

\end{frame} 

\begin{vbframe}{Decision regions and boundaries}
\begin{itemize}
  \item A \textbf{decision region} for class $k$ is the set of input points $\xv$ where class $k$ is assigned as prediction of our model:
$$
\Xspace_k = \{\xv \in \Xspace : \hx = k\}
$$

\item Points in space where the classes with maximal score are tied and the corresponding hypersurfaces are called \textbf{decision boundaries}
\end{itemize}

\begin{center}
  \includegraphics{figure_man/decision_boundaries.png} 
\end{center}
\end{vbframe} 

\begin{vbframe}{Decision Boundaries}
Formally:
\begin{eqnarray*}
\{ \xv \in \Xspace: && \exists~i \ne j \text{ s.t. } \fkx[i] = \fkx[j] \\ && \land \fkx[i], \fkx[j] \ge \fkx[k] ~ \forall k \ne i, j\}
\end{eqnarray*}  
  
In the binary case we can simplify and generalize to the decision boundary for general threshold $c$:

$$
    \{ \xv \in \Xspace : \fx = c \}
$$

If we set $c=0$ for scores and $c=0.5$ for probabilities, this is consistent with the definition above.

\end{vbframe}


\begin{vbframe}{Decision boundary examples}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_bdefs.pdf} 

}



\end{knitrout}

\end{vbframe}

\begin{vbframe}{Classification Approaches}

  Two fundamental approaches exist to construct classifiers:\\
  The \textbf{generative approach} and the \textbf{discriminant approach}.

\lz
They tackle the classification problem from different angles:

\begin{itemize}
\item \textbf{Generative} classification approaches assume a data-generating process in which the distribution of the features $\xv$ is different for the various classes of the output $y$, and try to learn these conditional distributions:\\ \enquote{Which $y$ tends to have $\xv$ like these?}
\lz
\item \textbf{Discriminant} approaches use \textbf{empirical risk minimization} based on a suitable loss function:\\ \enquote{What is the best prediction for $y$ given these $\xv$?}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Generative approach}

The \textbf{generative approach}
models $\pdfxyk$, usually by making some assumptions about the structure of these distributions, and employs the Bayes theorem:
$$\pikx = \postk = \frac{\P(\xv | y = k) \P(y = k)}{\P(\xv)} = \frac{\pdfxyk \pik}{\sumjg \pdfxyk[j] \pi_j}$$

The prior probabilities, $\pi_k = \P(y = k)$, for each class $k$ can be estimated from the training data as the relative frequency of each class:

\begin{center}
\includegraphics{figure_man/prior_probabilities.png} 
\end{center}

\end{vbframe}

\begin{vbframe}{Visualizing the generative approach}
The decision boundary between classes is implicitly defined by the overlapping regions of their probability distributions.

\begin{center}
\includegraphics[width=0.3\textwidth]{figure/approach_generative.png} 
\end{center}

\small{
Examples:
\begin{itemize}
\item Naive Bayes classifier
\item Linear discriminant analysis (generative, linear)
\item Quadratic discriminant analysis (generative, not linear)
\end{itemize}
}

{\footnotesize Note: LDA and QDA have 'discriminant' in their name, but are generative models!}
\end{vbframe}

\begin{vbframe}{Discriminant approach}

Here we optimize the discriminant functions directly, usually via empirical risk minimization, resulting in explicit decision boundaries:
$$ \fh = \argmin_{f \in \Hspace} \riskef = \argmin_{f \in \Hspace} \sumin \Lxyi.$$

\begin{center}
\includegraphics[width=0.3\textwidth]{figure/approach_discriminant.png} 
\end{center}

\small{
Examples are neural networks, logistic regression and support vector machines.
}

\end{vbframe}

\endlecture


\end{document}

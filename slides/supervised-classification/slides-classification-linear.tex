\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Linear Classifiers
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/linear_boundary.png
}{% Learning goals, wrapped inside itemize environment
  \item Linear classifier
  \item Linear decision boundaries
  \item Linear separability
}

\framebreak


\begin{vbframe}{Linear Classifiers}

Linear classifiers are an important subclass of classification models. 
If the discriminant function(s) $\fkx$ can be written as affine linear function(s) (possibly through a rank-preserving,
monotone transformation $g: \R \to \R$), i.e., 

$$
  g(\fkx) = \bm{w}_k^\top \xv + b_k,
$$

we will call the classifier a \textbf{linear classifier}. 

\vfill

NB: $\bm{w}_k$ and $b_k$ do not necessarily refer to the parameters $\thetav_k$, although they often coincide.

\end{vbframe}

  
\begin{vbframe}{linear decision boundaries}
  
We can also easily show that the decision boundary between classes $i$ and $j$ is a hyperplane. For every $\xv$ where there is a tie in scores: 

\begin{eqnarray*}
  \fkx[i] &=& \fkx[j] \\
  g(\fkx[i]) &=& g(\fkx[j]) \\
  \bm{w}_i^\top \xv + b_i &=& \bm{w}_j^\top \xv + b_j \\
  \left(\bm{w}_i - \bm{w}_j\right)^\top \xv + \left(b_i - b_j\right) &=& 0 
\end{eqnarray*}

This represents a \textbf{hyperplane} separating two classes:

\begin{center}
\includegraphics[width=0.33\textwidth]{figure_man/linear_boundary.png} 
\end{center}
\end{vbframe}

\begin{frame}{Example: 2 Classes with Centroids}


\only<1> {
\begin{itemize}
\item Let's model a binary problem by using a centroid $\muk$ per class as "parameters". 

\item  We don't really care how the centroids are estimated. We could estimate them by using class means, but the following doesn't depend on it.

\item Classify a point $\xv$ by assigning it to class $k$ of nearest centroid.

\end{itemize}
}

\only<2> {

%We classify a point $\xv$ by assigning it to the class $k$ whose centroid $\muk$ is closest (least distance $d$), i.e., we assign $\xv$ to class 1 if $d_1 < d_2$:
Let's calculate the boundary:
$$
d_1 = ||\xv - \muk[1]||^2 = \xv^\top \xv - 2 \xv^\top \muk[1] + \muk[1]^\top \muk[1]
= \xv^\top \xv - 2 \xv^\top \muk[2] + \muk[2]^\top \muk[2] = ||\xv - \muk[2]||^2 = d_2
$$

Where $d$ is measured using Euclidean distance. This implies:
$$
-2 \xv^\top \muk[1] + \muk[1]^\top \muk[1]
= -2 \xv^\top \muk[2] + \muk[2]^\top \muk[2]
$$

Which simplifies to:
$$
2 \xv^\top (\muk[2] - \muk[1]) =\muk[2]^\top \muk[2] - \muk[1]^\top \muk[1]
$$

Thus, it's a linear classifier!
}

\vspace{-0.85em}
\begin{center}
\includegraphics[width=0.9\textwidth]{figure/nearest_centroid_classifier.png} 
\end{center}

\end{frame}




\begin{vbframe}{linear separability}
If there exists a linear classifier that perfectly separates the classes of some dataset, the data are called \textbf{linearly separable}.

\vspace{1cm}

\begin{center}
\includegraphics{figure_man/linear_separability-1.png} 
\end{center}

\end{vbframe}


\begin{vbframe}{feature transformations}
Note that linear classifiers can represent \textbf{non-linear} decision boundaries in the original input space if we use derived features like higher order interactions, polynomial features, etc.

\begin{center}
\includegraphics{figure_man/linear_separability-2.png} 
\end{center}

\lz

Here we used absolute values to find suitable derived features.

\end{vbframe}

\endlecture

\end{document}

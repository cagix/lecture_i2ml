\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Linear Classifiers
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/linear_boundary.png
}{% Learning goals, wrapped inside itemize environment
  \item Know definition of linear classifier
  \item Understand the connection between linear classifiers and linear decision boundaries
  \item Grasp linear separability
}

\framebreak


\begin{vbframe}{Linear Classifiers}

Linear classifiers are an important subclass of classification models. 
If the discriminant function(s) $\fkx$ can be specified as linear function(s) (possibly through a rank-preserving,
monotone transformation $g: \R \to \R$), i.e., 

$$
  g(\fkx) = \bm{w}_k^\top \xv + b_k,
$$

we will call the classifier a \textbf{linear classifier}. 

\vfill

NB: $\bm{w}_k$ and $b_k$ do not directly refer to the parameters $\thetav_k$ 
of $k$-th scoring function $f_k$ but the transformed version. 

\end{vbframe}

\begin{vbframe}{Example}
We classify a point $\xv$ by assigning it to the class $k$ whose centroid $\muk$ is closest (least distance $d$), i.e., we assign $\xv$ to class 1 if $d_1 < d_2$:
$$
d_1 = ||\xv - \muk[1]||^2 = \xv^\top \xv - 2 \xv^\top \muk[1] + \muk[1]^\top \muk[1]
< \xv^\top \xv - 2 \xv^\top \muk[2] + \muk[2]^\top \muk[2] = ||\xv - \muk[2]||^2 = d_2
$$

Where $d$ is measured using Euclidean distance. This implies:
$$
-2 \xv^\top \muk[1] + \muk[1]^\top \muk[1]
< -2 \xv^\top \muk[2] + \muk[2]^\top \muk[2]
$$

Which simplifies to:
$$
2 \xv^\top (\muk[2] - \muk[1]) < \muk[2]^\top \muk[2] - \muk[1]^\top \muk[1]
$$

Thus, it's a linear classifier!
\vspace{-0.85em}
\begin{center}
\includegraphics[width=0.9\textwidth]{figure/nearest_centroid_classifier.png} 
\end{center}

\end{vbframe}
  
\begin{vbframe}{linear decision boundaries}
  
We can also easily show that the decision boundary between classes $i$ and $j$ is a hyperplane. For every $\xv$ where there is a tie in scores: 

\begin{eqnarray*}
  \fkx[i] &=& \fkx[j] \\
  g(\fkx[i]) &=& g(\fkx[j]) \\
  \bm{w}_i^\top \xv + b_i &=& \bm{w}_j^\top \xv + b_j \\
  \left(\bm{w}_i - \bm{w}_j\right)^\top \xv + \left(b_i - b_j\right) &=& 0 
\end{eqnarray*}

This represents a \textbf{hyperplane} separating two classes:

\begin{center}
\includegraphics[width=0.33\textwidth]{figure_man/linear_boundary.png} 
\end{center}


\end{vbframe}

\begin{vbframe}{linear separability}
If there exists a linear classifier that perfectly separates the classes of some dataset, the data are called \textbf{linearly separable}.

\vspace{1cm}

\begin{center}
\includegraphics{figure_man/linear_separability-1.png} 
\end{center}

\end{vbframe}


\begin{vbframe}{feature transformations}
Note that linear classifiers can represent \textbf{non-linear} decision boundaries in the original input space if we use derived features like higher order interactions, polynomial features, etc.

\begin{center}
\includegraphics{figure_man/linear_separability-2.png} 
\end{center}

\lz

Here we used absolute values to find suitable derived features.

\end{vbframe}

\endlecture

\end{document}

\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Naive Bayes
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/nb-db
}{% Learning goals, wrapped inside itemize environment
  \item Understand the idea of Naive Bayes
  \item Understand in which sense Naive Bayes is a special QDA model
}

\framebreak

\begin{vbframe}{Naive Bayes classifier}

NB is a generative multiclass technique. Remember: We use Bayes' theorem and only need $\pdfxyk$ to compute the posterior as:
$$\pikx \approx \postk = \frac{\P(\xv | y = k) \P(y = k)}{\P(\xv)} = \frac{\pdfxyk \pik}{\sumjg \pdfxyk[j] \pi_j} $$


NB is based on a simple \textbf{conditional independence assumption}: the features are conditionally independent given class $y$.
$$
\pdfxyk = p((x_1, x_2, ..., x_p)|y = k)=\prodjp p(x_j|y = k).
$$
So we only need to specify and estimate the distribution $p(x_j|y = k)$, which is considerably simpler as this is univariate.

\end{vbframe}


\begin{vbframe}{NB: Numerical Features}

We use a univariate Gaussian for $p(x_j | y=k)$, and estimate $(\mu_{kj}, \sigma^2_{kj})$ in the standard manner. Because of $\pdfxyk = \prodjp p(x_j|y = k)$, the joint conditional density is Gaussian with diagonal but non-isotropic covariance structure, and potentially different across classes.

\begin{center}
\includegraphics[width=0.79\textwidth, clip = true, trim = {0 20 0 20}]{figure/nb-db.png} 
\end{center}

$\Rightarrow$  \textbf{NB is a} (specific) \textbf{QDA model}, with a quadratic decision boundary.

\end{vbframe}
\begin{frame}{NB: Categorical Features}

We use a categorical distribution for $p(x_j | y = k)$ and estimate the probabilities $p_{kjm}$ that, in class $k$, our $j$-th feature has value $m$, $x_j = m$, simply by counting the frequencies.

$$
p(x_j | y = k) = \prod_m p_{kjm}^{[x_j = m]}
$$

Because of the simple conditional independence structure, it is also very easy to deal with mixed numerical / categorical feature spaces.

\begin{flushright}
% SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit?usp=sharing
\only<1>{\includegraphics[width=\textwidth, clip = true, trim = {50 400 120 350}]{figure_man/nb-categorial_1.png}}
% \only<2>{\includegraphics[width=\textwidth, clip = true, trim = {50 410 120 350}]{figure_man/nb-categorial_2.png}}
% \only<3>{\includegraphics[width=\textwidth, clip = true, trim = {50 410 120 350}]{figure_man/nb-categorial_3.png}}
\only<2>{\includegraphics[width=\textwidth, clip = true, trim = {50 410 120 350}]{figure_man/nb-categorial_final.png}}
\end{flushright}

\end{frame}

\begin{vbframe}{Laplace Smoothing: The Problem}
\begin{small}
If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero, e.g.:\\
$p_{\text{no, class, 1st}}^{[x_{class} = \text{1st}]} = 0$ (everyone from 1st class survived in the previous table)

\lz

When computing $\postk$, any zero probability will cause the entire product $p(x_{class} | y = no) = \prod_m p_{\text{no, class, m}}^{[x_{class} = m]}$ to be zero, negating information from other features:

$$
\P(\text{no} | \text{class = 1st, sex = male}) = \frac{p(x_{class} | y = no) \cdot p(x_{sex} | y = no)\cdot \pikh[no]}{\sumjg p(\text{class = 1st, sex = male} | y = j)\pih_j} = 0
$$

This is problematic because it will wipe out all information in the other probabilities when they are multiplied!

\lz

%$\Rightarrow$ A simple numerical correction is to set these zero probabilities to a small value to regularize against this case.

\end{small}

\end{vbframe}


\begin{vbframe}{Laplace Smoothing: The Solution}

A simple numerical correction is to set these zero probabilities to a small value to regularize against this case.
 
\begin{itemize}
\item To prevent zero probabilities, add a constant $\alpha > 0$ (e.g., $\alpha = 1$).
\item For a categorical feature $x_j$ with $M_j$ possible values:
  $$
  p_{kjm}^{[x_j = m]} = \frac{n_{kjm} + \alpha}{n_{k} + \alpha M_j} \quad \left(\text{instead of }  p_{kjm}^{[x_j = m]} = \frac{n_{kjm}}{n_{k}} \right)
  $$
  where:
  \begin{itemize}
    \item $n_{kjm}$: count of $x_j = m$ in class $k$,
    \item $n_{k}$: total counts in class $k$,
    \item $M_j$: number of possible distinct values of $x_j$.
  \end{itemize}
\item Ensures all probs are non-zero, avoiding the product to be zero.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Laplace Smoothing: Example}

With Laplace smoothing ($\alpha = 1$), we adjust:
$$
p_{\text{no, class, 1st}}^{[x_{class} = \text{1st}]} = \frac{n_{\text{no, class, 1st}} + 1}{n_{\text{class, 1st}} + 1 \cdot M_{class}},
$$
where $M_{class} = 3$ (= number of distinct values for $x_{class}$: 1st, 2nd, 3rd).

\lz


Even if $n_{\text{no, class, 1st}} = 0$ (everyone from 1st class survived), we get $p_{\text{no, class, 1st}} > 0$.
% $$
% \P(\text{no} | \text{class = 1st, sex = male}) = \frac{p_{\text{no, class, 1st}} \cdot p_{\text{no, sex, male}} \cdot \pikh[no]}{\sumjg p(\text{class = 1st, sex = male} | y = j)\pih_j} > 0.
% $$

\lz

This ensures that the posterior probability is non-zero, preserving the influence of all features in the model.


\end{vbframe}



\begin{vbframe}{Naive Bayes: application as spam filter}
\begin{itemize}
  \item In the late 90s, Naive Bayes became popular for e-mail spam filter programs
  \item Word counts were used as features to detect spam mails (e.g., "Viagra" often occurs in spam mail)
  \item Independence assumption implies: occurrence of two words in mail is not correlated
  \item Seems naive ("Viagra" more likely to occur in context with "Buy now" than "flower"), but leads to less required parameters and therefore better generalization.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Benchmarking SPAM}
Benchmarking QDA, Naive Bayes and LDA on $\texttt{spam}$:

\begin{center}
\includegraphics[clip=true, trim={0 0 0 17}, width=0.80\linewidth]{figure/nb-bench.png}
\end{center}

$\Rightarrow$ In practice, NB often performs well even when the independence assumption is violated!
\end{vbframe}

\endlecture

\end{document}

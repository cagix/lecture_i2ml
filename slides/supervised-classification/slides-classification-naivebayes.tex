\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Naive Bayes
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/nb-db
}{% Learning goals, wrapped inside itemize environment
  \item Understand the idea of Naive Bayes
  \item Understand in which sense Naive Bayes is a special QDA model
}

\framebreak

\begin{vbframe}{Naive Bayes classifier}

NB is a generative multiclass technique. Remember: We use Bayes' theorem and only need $\pdfxyk$ to compute the posterior as:
$$\pikx = \postk = \frac{\P(\xv | y = k) \P(y = k)}{\P(\xv)} = \frac{\pdfxyk \pik}{\sumjg \pdfxyk[j] \pi_j} $$


NB is based on a simple \textbf{conditional independence assumption}: the features are conditionally independent given class $y$.
$$
\pdfxyk = p((x_1, x_2, ..., x_p)|y = k)=\prodjp p(x_j|y = k).
$$
So we only need to specify and estimate the distribution $p(x_j|y = k)$, which is considerably simpler as this is univariate.

\end{vbframe}


\begin{vbframe}{NB: Numerical Features}

We use a univariate Gaussian for $p(x_j | y=k)$, and estimate $(\mu_{kj}, \sigma^2_{kj})$ in the standard manner. Because of $\pdfxyk = \prodjp p(x_j|y = k)$, the joint conditional density is Gaussian with diagonal but non-isotropic covariance structure, and potentially different across classes.

\begin{center}
\includegraphics[width=0.79\textwidth, clip = true, trim = {0 20 0 20}]{figure/nb-db.png} 
\end{center}

$\Rightarrow$  \textbf{NB is a} (specific) \textbf{QDA model}, with a quadratic decision boundary.

\end{vbframe}
\begin{frame}{NB: Categorical Features}

We use a categorical distribution for $p(x_j | y = k)$ and estimate the probabilities $p_{kjm}$ that, in class $k$, our $j$-th feature has value $m$, $x_j = m$, simply by counting the frequencies.

$$
p(x_j | y = k) = \prod_m p_{kjm}^{[x_j = m]}
$$

Because of the simple conditional independence structure it is also very easy to deal with mixed numerical / categorical feature spaces.

\begin{flushright}
% SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit?usp=sharing
\only<1>{\includegraphics[width=\textwidth, clip = true, trim = {50 400 120 350}]{figure_man/nb-categorial_1.png}}
\only<2>{\includegraphics[width=\textwidth, clip = true, trim = {50 410 120 350}]{figure_man/nb-categorial_2.png}}
\only<3>{\includegraphics[width=\textwidth, clip = true, trim = {50 410 120 350}]{figure_man/nb-categorial_3.png}}
\end{flushright}

\end{frame}

\begin{vbframe}{Laplace Smoothing}
\begin{small}
If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero, e.g., $p_{\text{no, class, 1st}} = 0$.

\lz

When computing $\postk$, any zero probability will cause the entire product to be zero, negating information from other features:

$$
\P(\text{no} | \text{class = 1st, sex = male}) = \frac{p_{\text{no, class, 1st}} \cdot p_{\text{no, sex, male}} \cdot \pikh[no]}{\sumjg p(\text{class = 1st, sex = male} | y = j)\pih_j} = 0
$$

This is problematic because it will wipe out all information in the other probabilities when they are multiplied!

\lz

$\Rightarrow$ A simple numerical correction is to set these zero probabilities to a small value to regularize against this case.

\end{small}

\end{vbframe}

\begin{vbframe}{Naive Bayes: application as spam filter}
\begin{itemize}
  \item In the late 90s, Naive Bayes became popular for e-mail spam filter programs
  \item Word counts were used as features to detect spam mails (e.g., "Viagra" often occurs in spam mail)
  \item Independence assumption implies: occurrence of two words in mail is not correlated
  \item Seems naive ("Viagra" more likely to occur in context with "Buy now" than "flower"), but leads to less required parameters and therefore better generalization.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Benchmarking SPAM}
Benchmarking QDA, Naive Bayes and LDA on $\texttt{spam}$:

\begin{center}
\includegraphics[clip=true, trim={0 0 0 17}, width=0.80\linewidth]{figure/nb-bench.png}
\end{center}

$\Rightarrow$ In practice, NB often performs well even when the independence assumption is violated!
\end{vbframe}

\endlecture

\end{document}

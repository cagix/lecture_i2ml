Linear classifiers like logistic regression learn a decision boundary that takes the form of a (linear) hyperplane.
Hyperplanes are defined by equations $\thx = b$ with coefficients $\thetab$ and a scalar $b \in \R$.

In order to see that such expressions actually describe hyperplanes, consider $\thx = \theta_0 + \theta_1 x_1 + \theta_2 x_2 = 0$.
Sketch the hyperplanes given by the following coefficients and explain the difference between the parameterizations:
\begin{itemize}
\item $\theta_0 = 0, \theta_1 = \theta_2 = 1$
\item $\theta_0 = 1, \theta_1 = \theta_2 = 1$
\item $\theta_0 = 0, \theta_1 = 1, \theta_2 = 2$
\end{itemize}

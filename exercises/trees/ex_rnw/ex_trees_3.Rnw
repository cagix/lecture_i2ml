We will now build some intuition for the Brier score / Gini impurity as a 
splitting criterion by showing that it is equal to the expected 
MCE of the resulting node.

The fractions of the classes $k=1,\ldots, g$ in node $\Np$ of a decision 
tree are $\pi^{(\Np)}_1,\ldots,\pi^{(\Np)}_g$, where $$ \pikN = \frac{1}{|\Np|} 
\sum_{(x^{(i)},y^{(i)}) \in \Np} [y^{(i)} = k].$$

For an expression that holds in expectation over arbitrary data, we need to 
introduce stochasticity.
Assume we replace the (deterministic) classification rule in node $\Np$
$$\hat{k}~|~\Np=\arg\max_k \pikN$$
by a randomizing rule
$$\hat k \sim \text{Cat} \left(\pi^{(\Np)}_1,\ldots,\pi^{(\Np)}_g \right),$$ 
in which we draw the classes from the categorical distribution of 
their estimated probabilities (i.e., class $k$ is predicted with probability 
$\pi^{(\Np)}_k$).

\begin{enumerate}[a)]
  \item Explain the difference between the deterministic and the randomized classification rule.
  \item Using the randomized rule, compute the expected MCE in node $\Np$ that contains $n$ random training samples.
What do you notice?
\end{enumerate}


% (\textit{Hint}: The observations and the predictions using 
% the randomizing rule follow the same distribution.)